{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/secorec/anaconda3/envs/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278800, 576)\n",
      "(383836, 576)\n",
      "(586322, 24, 24, 1)\n",
      "(469057, 24, 24, 1)\n",
      "(117265, 24, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Activation\n",
    "from keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers import GaussianNoise as GN\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learnRate = 0.01\n",
    "\n",
    "# Learning rate annealing\n",
    "def step_decay(epoch):\n",
    "    if epoch/epochs<0.3:\n",
    "        lrate = learnRate\n",
    "    elif epoch/epochs<=0.5:\n",
    "        lrate = learnRate/2\n",
    "    elif epoch/epochs<=0.70:\n",
    "        lrate = learnRate/10\n",
    "    else:\n",
    "        lrate = learnRate/100\n",
    "    return lrate\n",
    "\n",
    "\n",
    "\"\"\"Load Data\"\"\"\n",
    "img_rows, img_cols = 24, 24\n",
    "faces = np.loadtxt('data/dfFaces_24x24_norm')\n",
    "x = np.load(\"data/extra_faces.npy\")\n",
    "x = x.reshape(x.shape[0],576)\n",
    "faces = np.concatenate((faces,x),axis=0)\n",
    "\n",
    "\n",
    "notfaces = np.loadtxt('data/NotFaces_24x24_norm')\n",
    "x = np.load(\"data/extra_NotFaces.npy\")\n",
    "x = x.reshape(x.shape[0],576)\n",
    "notfaces = np.concatenate((notfaces,x),axis=0)\n",
    "print(notfaces.shape)\n",
    "x = np.load(\"data/not_faces_2extras.npy\")\n",
    "x = x.reshape(x.shape[0],576)\n",
    "notfaces = np.concatenate((notfaces,x),axis=0)\n",
    "print(notfaces.shape)\n",
    "\n",
    "yfaces = np.ones(faces.shape[0])\n",
    "yNotfaces = np.zeros(notfaces.shape[0])\n",
    "\n",
    "y = np.append(yfaces, yNotfaces)\n",
    "x = np.concatenate((faces, notfaces), axis=0)\n",
    "\n",
    "np.random.seed(1992)\n",
    "aux_list = list(zip(x, y))\n",
    "random.shuffle(aux_list)\n",
    "x, y = zip(*aux_list)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "x = x.reshape(x.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1992)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 24, 24, 1)         4         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 182,277\n",
      "Trainable params: 181,507\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3665/3664 [==============================] - 38s 10ms/step - loss: 0.1567 - acc: 0.9437 - val_loss: 0.0457 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04572, saving model to temp.hdf5\n",
      "Epoch 2/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.1254 - acc: 0.9562 - val_loss: 0.0624 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.1165 - acc: 0.9594 - val_loss: 0.0423 - val_acc: 0.9885\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04572 to 0.04234, saving model to temp.hdf5\n",
      "Epoch 4/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.1045 - acc: 0.9649 - val_loss: 0.0342 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04234 to 0.03423, saving model to temp.hdf5\n",
      "Epoch 5/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.1006 - acc: 0.9660 - val_loss: 0.0331 - val_acc: 0.9892\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03423 to 0.03314, saving model to temp.hdf5\n",
      "Epoch 6/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0977 - acc: 0.9676 - val_loss: 0.0386 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0830 - acc: 0.9719 - val_loss: 0.0285 - val_acc: 0.9915\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03314 to 0.02847, saving model to temp.hdf5\n",
      "Epoch 8/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0759 - acc: 0.9744 - val_loss: 0.0285 - val_acc: 0.9920\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02847 to 0.02845, saving model to temp.hdf5\n",
      "Epoch 9/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0799 - acc: 0.9739 - val_loss: 0.0426 - val_acc: 0.9879\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0755 - acc: 0.9745 - val_loss: 0.0291 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0740 - acc: 0.9745 - val_loss: 0.0256 - val_acc: 0.9919\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02845 to 0.02557, saving model to temp.hdf5\n",
      "Epoch 12/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0664 - acc: 0.9776 - val_loss: 0.0253 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02557 to 0.02533, saving model to temp.hdf5\n",
      "Epoch 13/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0658 - acc: 0.9775 - val_loss: 0.0260 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0632 - acc: 0.9779 - val_loss: 0.0253 - val_acc: 0.9929\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02533 to 0.02526, saving model to temp.hdf5\n",
      "Epoch 15/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0631 - acc: 0.9786 - val_loss: 0.0258 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0632 - acc: 0.9786 - val_loss: 0.0249 - val_acc: 0.9927\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02526 to 0.02495, saving model to temp.hdf5\n",
      "Epoch 17/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0627 - acc: 0.9791 - val_loss: 0.0251 - val_acc: 0.9929\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0619 - acc: 0.9789 - val_loss: 0.0248 - val_acc: 0.9928\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02495 to 0.02482, saving model to temp.hdf5\n",
      "Epoch 19/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0630 - acc: 0.9791 - val_loss: 0.0250 - val_acc: 0.9928\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "3665/3664 [==============================] - 36s 10ms/step - loss: 0.0594 - acc: 0.9798 - val_loss: 0.0248 - val_acc: 0.9929\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02482 to 0.02478, saving model to temp.hdf5\n",
      "117265/117265 [==============================] - 11s 93us/step\n",
      "Test loss: 0.024777135113490466\n",
      "Test accuracy: 0.7095041145613767 %\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "_________________Test Confusion Matrix_________________\n",
      "[[76262   386]\n",
      " [  446 40171]]\n",
      "______________________Test Report______________________\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.99      0.99     76648\n",
      "        1.0       0.99      0.99      0.99     40617\n",
      "\n",
      "avg / total       0.99      0.99      0.99    117265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\"\"\"Data Generator\"\"\"\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            horizontal_flip=False,\n",
    "                            rotation_range=20,\n",
    "                            zoom_range=[1.0,1.2],\n",
    "                            vertical_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\"\"\"Create Model for training\"\"\"\n",
    "shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BN(input_shape=shape))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BN())\n",
    "model.add(GN(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, kernel_initializer=glorot_uniform(seed=0)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BN())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, kernel_initializer=glorot_uniform(seed=0)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BN())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "adam = Adam(lr=learnRate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint_path = \"data/temp.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train),\n",
    "                    steps_per_epoch = len(x_train)/batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[lrate, checkpointer])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', (1-score[1])*100,'%')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred)\n",
    "print(\"\")\n",
    "print(\"_________________Test Confusion Matrix_________________\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"______________________Test Report______________________\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "model.save('model_extraData_3.h5')\n",
    "model.save_weights('weights_model_extraData_3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
