{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/secorec/anaconda3/envs/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278800, 576)\n",
      "(383836, 576)\n",
      "(586322, 24, 24, 1)\n",
      "(469057, 24, 24, 1)\n",
      "(117265, 24, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense, Activation\n",
    "from keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "from keras.layers import GaussianNoise as GN\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "learnRate = 0.01\n",
    "\n",
    "# Learning rate annealing\n",
    "def step_decay(epoch):\n",
    "    if epoch/epochs<0.3:\n",
    "        lrate = learnRate\n",
    "    elif epoch/epochs<=0.5:\n",
    "        lrate = learnRate/2\n",
    "    elif epoch/epochs<=0.70:\n",
    "        lrate = learnRate/10\n",
    "    else:\n",
    "        lrate = learnRate/100\n",
    "    return lrate\n",
    "\n",
    "\n",
    "\"\"\"Load Data\"\"\"\n",
    "img_rows, img_cols = 24, 24\n",
    "faces = np.loadtxt('data/dfFaces_24x24_norm')\n",
    "x = np.load(\"data/extra_faces.npy\")\n",
    "x = x.reshape(x.shape[0],576)\n",
    "faces = np.concatenate((faces,x),axis=0)\n",
    "\n",
    "\n",
    "notfaces = np.loadtxt('data/NotFaces_24x24_norm')\n",
    "x = np.load(\"data/extra_NotFaces.npy\")\n",
    "x = x.reshape(x.shape[0],576)\n",
    "notfaces = np.concatenate((notfaces,x),axis=0)\n",
    "print(notfaces.shape)\n",
    "x = np.load(\"data/not_faces_2extras.npy\")\n",
    "x = x.reshape(x.shape[0],576)\n",
    "notfaces = np.concatenate((notfaces,x),axis=0)\n",
    "print(notfaces.shape)\n",
    "\n",
    "yfaces = np.ones(faces.shape[0])\n",
    "yNotfaces = np.zeros(notfaces.shape[0])\n",
    "\n",
    "y = np.append(yfaces, yNotfaces)\n",
    "x = np.concatenate((faces, notfaces), axis=0)\n",
    "\n",
    "np.random.seed(1992)\n",
    "aux_list = list(zip(x, y))\n",
    "random.shuffle(aux_list)\n",
    "x, y = zip(*aux_list)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "x = x.reshape(x.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1992)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_4 (Batch (None, 24, 24, 1)         4         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 24, 24, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,214,917\n",
      "Trainable params: 1,214,083\n",
      "Non-trainable params: 834\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "3665/3664 [==============================] - 44s 12ms/step - loss: 0.1120 - acc: 0.9603 - val_loss: 0.0246 - val_acc: 0.9924\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02458, saving model to data/temp.hdf5\n",
      "Epoch 2/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0796 - acc: 0.9728 - val_loss: 0.0206 - val_acc: 0.9935\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02458 to 0.02060, saving model to data/temp.hdf5\n",
      "Epoch 3/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0686 - acc: 0.9770 - val_loss: 0.0158 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02060 to 0.01581, saving model to data/temp.hdf5\n",
      "Epoch 4/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0635 - acc: 0.9782 - val_loss: 0.0164 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0630 - acc: 0.9788 - val_loss: 0.0238 - val_acc: 0.9900\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0542 - acc: 0.9815 - val_loss: 0.0179 - val_acc: 0.9947\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0471 - acc: 0.9845 - val_loss: 0.0135 - val_acc: 0.9955\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01581 to 0.01346, saving model to data/temp.hdf5\n",
      "Epoch 8/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0460 - acc: 0.9847 - val_loss: 0.0129 - val_acc: 0.9958\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01346 to 0.01286, saving model to data/temp.hdf5\n",
      "Epoch 9/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0440 - acc: 0.9848 - val_loss: 0.0127 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01286 to 0.01272, saving model to data/temp.hdf5\n",
      "Epoch 10/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0419 - acc: 0.9858 - val_loss: 0.0143 - val_acc: 0.9953\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0391 - acc: 0.9872 - val_loss: 0.0116 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01272 to 0.01164, saving model to data/temp.hdf5\n",
      "Epoch 12/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0353 - acc: 0.9884 - val_loss: 0.0100 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01164 to 0.01003, saving model to data/temp.hdf5\n",
      "Epoch 13/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0339 - acc: 0.9887 - val_loss: 0.0175 - val_acc: 0.9962\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0317 - acc: 0.9893 - val_loss: 0.0104 - val_acc: 0.9969\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0317 - acc: 0.9894 - val_loss: 0.0113 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0310 - acc: 0.9895 - val_loss: 0.0128 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0299 - acc: 0.9898 - val_loss: 0.0132 - val_acc: 0.9968\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0325 - acc: 0.9893 - val_loss: 0.0145 - val_acc: 0.9966\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0299 - acc: 0.9898 - val_loss: 0.0130 - val_acc: 0.9967\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "3665/3664 [==============================] - 43s 12ms/step - loss: 0.0304 - acc: 0.9897 - val_loss: 0.0141 - val_acc: 0.9965\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "117265/117265 [==============================] - 11s 98us/step\n",
      "Test loss: 0.014079927663188073\n",
      "Test accuracy: 0.3470771329385558 %\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "_________________Test Confusion Matrix_________________\n",
      "[[76404   244]\n",
      " [  163 40454]]\n",
      "______________________Test Report______________________\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00     76648\n",
      "        1.0       0.99      1.00      0.99     40617\n",
      "\n",
      "avg / total       1.00      1.00      1.00    117265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\"\"\"Data Generator\"\"\"\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            horizontal_flip=False,\n",
    "                            rotation_range=20,\n",
    "                            zoom_range=[1.0,1.2],\n",
    "                            vertical_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\"\"\"Create Model for training\"\"\"\n",
    "shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BN(input_shape=shape))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer=glorot_uniform(seed=0)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BN())\n",
    "model.add(GN(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, kernel_initializer=glorot_uniform(seed=0)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BN())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, kernel_initializer=glorot_uniform(seed=0)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BN())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "adam = Adam(lr=learnRate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint_path = \"data/temp.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train),\n",
    "                    steps_per_epoch = len(x_train)/batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[lrate, checkpointer])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', (1-score[1])*100,'%')\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(y_pred)\n",
    "print(\"\")\n",
    "print(\"_________________Test Confusion Matrix_________________\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"______________________Test Report______________________\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "model.save('model_extraData_3.h5')\n",
    "model.save_weights('weights_model_extraData_3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
